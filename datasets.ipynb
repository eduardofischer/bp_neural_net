{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Requisitos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neural_net as nn\n",
    "import math"
   ]
  },
  {
   "source": [
    "## Funções Auxiliares"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geração de conjuntos estratificados para validação cruzada\n",
    "def k_folds(df, target_attr, k=5, shuffle_seed=99):\n",
    "    # Número de classes (valores distintos na coluna alvo)\n",
    "    unique_classes = df[target_attr].unique()\n",
    "    # Separa o dataset em 2 de acordo com o valor do atributo alvo e embaralha as entradas dentro de cada subset\n",
    "    data_by_class = [df.loc[df[target_attr] == c].sample(frac=1, random_state=shuffle_seed) for c in unique_classes]\n",
    "    # Cria uma lista com k dataframes\n",
    "    folds = [pd.DataFrame() for i in range(k)]\n",
    "    # Divide os dados em k dataframes de forma estratificada\n",
    "    for class_data in data_by_class:\n",
    "        n_rows = class_data.iloc[:, -1].count()\n",
    "        fold_size = math.ceil(n_rows / k)\n",
    "        for i in range(k):\n",
    "            folds[i] = folds[i].append(class_data.iloc[i*fold_size:(i+1)*fold_size])\n",
    "    # Embaralha as instâncias dentro de cada fold\n",
    "    for i in range(len(folds)): folds[i] = folds[i].sample(frac=1, random_state=shuffle_seed)\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dada uma lista, retorna a lista sem o elemento de index i\n",
    "def remove_index(list, index):\n",
    "    if len(list) - 1 > index:\n",
    "        return list[:index] + list[(index+1):]\n",
    "    elif len(list) - 1 == index and index >= 0:\n",
    "        return list[:index]\n",
    "    else: raise Exception('index inválido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna uma tupla que lista os atributos preditivos no formato (attr_name, attr_type, [possible_values])\n",
    "# a partir de um arquivo csv \"file\" com separador \"sep\" no formato \n",
    "# \"attr1<sep>attr2  \n",
    "# attr1_type<sep>attr2_type\n",
    "# value1<sep>value2\"\n",
    "def read_attr_list(file, sep):\n",
    "  attr_df = pd.read_csv(file, sep=sep)\n",
    "\n",
    "  attr_list = []\n",
    "\n",
    "  for col in range(len(attr_df.columns)):\n",
    "    attr_tuple = (attr_df.columns[col], attr_df.iloc[0, col])\n",
    "    if len(list(attr_df[attr_df.columns[col]].dropna().drop([0]))) != 0:\n",
    "      attr_tuple += (list(attr_df[attr_df.columns[col]].dropna().drop([0])),)\n",
    "    attr_list.append(attr_tuple)\n",
    "\n",
    "  return attr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina uma rede neural, testa um conjunto de teste e retorna a acurácia\n",
    "def test_neural_net(training_df, testing_df, attr_list, target_attr=\"target\", alpha=0.05, lamb=0.0, intermediate_net=[2], n_outputs=1):\n",
    "    correct_predictions = 0\n",
    "    incorrect_predictions = 0\n",
    "\n",
    "    network = [len(attr_list), *intermediate_net, n_outputs]\n",
    "    print(network)\n",
    "\n",
    "    net = nn.NeuralNetwork(network, alpha=0.05, lamb=0.0)\n",
    "\n",
    "    # TODO\n",
    "    inputs = []\n",
    "    exp_outputs = []\n",
    "\n",
    "    net.train(inputs, exp_outputs)\n",
    "\n",
    "    for idx, instance in testing_df.iterrows():\n",
    "      predicted_class = net.classify(instance_in, instance_out, n_outputs)\n",
    "      actual_class = testing_df.at[idx, target_attr]\n",
    "      if actual_class == predicted_class:\n",
    "        correct_predictions += 1\n",
    "      else:\n",
    "        incorrect_predictions += 1\n",
    "    accuracy = round(correct_predictions / testing_df.shape[0], 2)\n",
    "    return accuracy"
   ]
  },
  {
   "source": [
    "## Dataset 1 - house-votes-84.tsv"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[16, 8, 1]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (0,) (8,17) ",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-69cabdaac543>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mtesting_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_neural_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintermediate_net\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mfolds_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2eb7af84906b>\u001b[0m in \u001b[0;36mtest_neural_net\u001b[0;34m(training_df, testing_df, attr_list, target_attr, alpha, lamb, intermediate_net, n_outputs)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mexp_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtesting_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/bp_neural_net/neural_net.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs_array, outputs_array, n_ephocs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_regularize_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/bp_neural_net/neural_net.py\u001b[0m in \u001b[0;36m_regularize_gradients\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mneuron\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0;31m# Calcula os pesos da rede\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (0,) (8,17) "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/house-votes-84.tsv', sep='\\t')\n",
    "\n",
    "target_attr = \"target\"\n",
    "n_folds = 5\n",
    "\n",
    "# Gera conjuntos estratificados para a validação cruzada\n",
    "folds = k_folds(df, target_attr, n_folds)\n",
    "attr_list = read_attr_list(\"datasets/house-votes-attr.csv\", \";\")\n",
    "folds_accuracy = []\n",
    "\n",
    "# Teste da rede neural para cada um dos folds\n",
    "for i in range(len(folds)):\n",
    "  training_df = pd.concat(remove_index(folds, i))\n",
    "  testing_df = folds[i]\n",
    "\n",
    "  accuracy = test_neural_net(training_df, testing_df, attr_list, target_attr, alpha=0.05, lamb=0.0, intermediate_net=[8])\n",
    "  folds_accuracy.append([accuracy])\n",
    "\n",
    "# Agregação dos Resultados\n",
    "stats = pd.DataFrame(folds_accuracy, columns=[\"Accuracy\"])\n",
    "desc = stats.describe()\n",
    "results = pd.concat([stats, desc.loc[['mean', 'std']] ])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}