{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Requisitos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neural_net as nn\n",
    "import math"
   ]
  },
  {
   "source": [
    "## Funções Auxiliares"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geração de conjuntos estratificados para validação cruzada\n",
    "def k_folds(df, target_attr, k=5, shuffle_seed=99):\n",
    "    # Número de classes (valores distintos na coluna alvo)\n",
    "    unique_classes = df[target_attr].unique()\n",
    "    # Separa o dataset em 2 de acordo com o valor do atributo alvo e embaralha as entradas dentro de cada subset\n",
    "    data_by_class = [df.loc[df[target_attr] == c].sample(frac=1, random_state=shuffle_seed) for c in unique_classes]\n",
    "    # Cria uma lista com k dataframes\n",
    "    folds = [pd.DataFrame() for i in range(k)]\n",
    "    # Divide os dados em k dataframes de forma estratificada\n",
    "    for class_data in data_by_class:\n",
    "        n_rows = class_data.iloc[:, -1].count()\n",
    "        fold_size = math.ceil(n_rows / k)\n",
    "        for i in range(k):\n",
    "            folds[i] = folds[i].append(class_data.iloc[i*fold_size:(i+1)*fold_size])\n",
    "    # Embaralha as instâncias dentro de cada fold\n",
    "    for i in range(len(folds)): folds[i] = folds[i].sample(frac=1, random_state=shuffle_seed)\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dada uma lista, retorna a lista sem o elemento de index i\n",
    "def remove_index(list, index):\n",
    "    if len(list) - 1 > index:\n",
    "        return list[:index] + list[(index+1):]\n",
    "    elif len(list) - 1 == index and index >= 0:\n",
    "        return list[:index]\n",
    "    else: raise Exception('index inválido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna uma tupla que lista os atributos preditivos no formato (attr_name, attr_type, [possible_values])\n",
    "# a partir de um arquivo csv \"file\" com separador \"sep\" no formato \n",
    "# \"attr1<sep>attr2  \n",
    "# attr1_type<sep>attr2_type\n",
    "# value1<sep>value2\"\n",
    "def read_attr_list(file, sep):\n",
    "  attr_df = pd.read_csv(file, sep=sep)\n",
    "\n",
    "  attr_list = []\n",
    "\n",
    "  for col in range(len(attr_df.columns)):\n",
    "    attr_tuple = (attr_df.columns[col], attr_df.iloc[0, col])\n",
    "    if len(list(attr_df[attr_df.columns[col]].dropna().drop([0]))) != 0:\n",
    "      attr_tuple += (list(attr_df[attr_df.columns[col]].dropna().drop([0])),)\n",
    "    attr_list.append(attr_tuple)\n",
    "\n",
    "  return attr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina uma rede neural, testa um conjunto de teste e retorna a acurácia\n",
    "def test_neural_net(training_df, testing_df, attr_list, target_attr=\"target\", alpha=0.05, lamb=0.0, intermediate_net=[2], n_outputs=1, n_epochs=100):\n",
    "    correct_predictions = 0\n",
    "    incorrect_predictions = 0\n",
    "\n",
    "    dataset = []\n",
    "    network = [len(attr_list), *intermediate_net, n_outputs]\n",
    "    net = nn.NeuralNetwork(network, alpha=0.05, lamb=0.0)\n",
    "\n",
    "    for idx, instance in training_df.iterrows(): \n",
    "      dataset.append([\n",
    "        instance.drop(target_attr).to_numpy(),\n",
    "        [instance[target_attr]]\n",
    "        ])\n",
    "\n",
    "    net.train(dataset, n_epochs)\n",
    "\n",
    "    for idx, instance in testing_df.iterrows():\n",
    "      inputs = instance.drop(target_attr).to_numpy()\n",
    "      predicted_class = net.classify(inputs, n_outputs)\n",
    "      actual_class = testing_df.at[idx, target_attr]\n",
    "      if actual_class == predicted_class:\n",
    "        correct_predictions += 1\n",
    "      else:\n",
    "        incorrect_predictions += 1\n",
    "    accuracy = round(correct_predictions / testing_df.shape[0], 2)\n",
    "    print(correct_predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "source": [
    "## Dataset 1 - house-votes-84.tsv"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9\n",
      "9\n",
      "10\n",
      "8\n",
      "5\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Accuracy\n",
       "0     0.820000\n",
       "1     0.820000\n",
       "2     0.910000\n",
       "3     0.730000\n",
       "4     0.830000\n",
       "mean  0.822000\n",
       "std   0.063797"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.820000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.820000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.910000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.730000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.830000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.822000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.063797</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "raw_df = pd.read_csv('datasets/house-votes-84.tsv', sep='\\t')\n",
    "\n",
    "# Normaliza o dataset\n",
    "df = (raw_df - raw_df.min()) / (raw_df.max() - raw_df.min())\n",
    "\n",
    "# Reduz o dataset pra fins de teste\n",
    "df = df.sample(50)\n",
    "\n",
    "target_attr = \"target\"\n",
    "n_folds = 5\n",
    "\n",
    "# Gera conjuntos estratificados para a validação cruzada\n",
    "folds = k_folds(df, target_attr, n_folds)\n",
    "attr_list = read_attr_list(\"datasets/house-votes-attr.csv\", \";\")\n",
    "folds_accuracy = []\n",
    "\n",
    "# Teste da rede neural para cada um dos folds\n",
    "for i in range(len(folds)):\n",
    "  training_df = pd.concat(remove_index(folds, i))\n",
    "  testing_df = folds[i]\n",
    "\n",
    "  accuracy = test_neural_net(training_df, testing_df, attr_list, target_attr, alpha=0.005, lamb=0.25, intermediate_net=[12], n_epochs=1000)\n",
    "  folds_accuracy.append([accuracy])\n",
    "\n",
    "# Agregação dos Resultados\n",
    "stats = pd.DataFrame(folds_accuracy, columns=[\"Accuracy\"])\n",
    "desc = stats.describe()\n",
    "results = pd.concat([stats, desc.loc[['mean', 'std']] ])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}