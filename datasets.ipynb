{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Requisitos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neural_net as nn\n",
    "import math"
   ]
  },
  {
   "source": [
    "## Funções Auxiliares"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geração de conjuntos estratificados para validação cruzada\n",
    "def k_folds(df, target_attr, k=5, shuffle_seed=99):\n",
    "    # Número de classes (valores distintos na coluna alvo)\n",
    "    unique_classes = df[target_attr].unique()\n",
    "    # Separa o dataset em 2 de acordo com o valor do atributo alvo e embaralha as entradas dentro de cada subset\n",
    "    data_by_class = [df.loc[df[target_attr] == c].sample(frac=1, random_state=shuffle_seed) for c in unique_classes]\n",
    "    # Cria uma lista com k dataframes\n",
    "    folds = [pd.DataFrame() for i in range(k)]\n",
    "    # Divide os dados em k dataframes de forma estratificada\n",
    "    for class_data in data_by_class:\n",
    "        n_rows = class_data.iloc[:, -1].count()\n",
    "        fold_size = math.ceil(n_rows / k)\n",
    "        for i in range(k):\n",
    "            folds[i] = folds[i].append(class_data.iloc[i*fold_size:(i+1)*fold_size])\n",
    "    # Embaralha as instâncias dentro de cada fold\n",
    "    for i in range(len(folds)): folds[i] = folds[i].sample(frac=1, random_state=shuffle_seed)\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dada uma lista, retorna a lista sem o elemento de index i\n",
    "def remove_index(list, index):\n",
    "    if len(list) - 1 > index:\n",
    "        return list[:index] + list[(index+1):]\n",
    "    elif len(list) - 1 == index and index >= 0:\n",
    "        return list[:index]\n",
    "    else: raise Exception('index inválido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna uma tupla que lista os atributos preditivos no formato (attr_name, attr_type, [possible_values])\n",
    "# a partir de um arquivo csv \"file\" com separador \"sep\" no formato \n",
    "# \"attr1<sep>attr2  \n",
    "# attr1_type<sep>attr2_type\n",
    "# value1<sep>value2\"\n",
    "def read_attr_list(file, sep):\n",
    "  attr_df = pd.read_csv(file, sep=sep)\n",
    "\n",
    "  attr_list = []\n",
    "\n",
    "  for col in range(len(attr_df.columns)):\n",
    "    attr_tuple = (attr_df.columns[col], attr_df.iloc[0, col])\n",
    "    if len(list(attr_df[attr_df.columns[col]].dropna().drop([0]))) != 0:\n",
    "      attr_tuple += (list(attr_df[attr_df.columns[col]].dropna().drop([0])),)\n",
    "    attr_list.append(attr_tuple)\n",
    "\n",
    "  return attr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(encoding, value):\n",
    "    hot = encoding.index(value)\n",
    "    out = [0 for _ in range(len(encoding) - 1)]\n",
    "    out.insert(hot, 1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina uma rede neural, testa um conjunto de teste e retorna a acurácia\n",
    "def test_neural_net(training_df, testing_df, attr_list, target_attr=\"target\", alpha=0.05, lamb=0.0, intermediate_net=[2], n_outputs=1, encoding=[], n_epochs=100):\n",
    "    correct_predictions = 0\n",
    "    incorrect_predictions = 0\n",
    "\n",
    "    training_set = []\n",
    "    testing_set = []\n",
    "    network = [len(attr_list), *intermediate_net, n_outputs]\n",
    "    net = nn.NeuralNetwork(network, alpha=0.05, lamb=0.0)\n",
    "\n",
    "    # Faz o parse do dataset de treinamento para o formato de entrada da rede\n",
    "    if n_outputs == 1:\n",
    "      for idx, instance in training_df.iterrows(): \n",
    "        training_set.append([\n",
    "          instance.drop(target_attr).to_numpy(),\n",
    "          [instance[target_attr]]\n",
    "          ])\n",
    "    else:\n",
    "      for idx, instance in training_df.iterrows(): \n",
    "        training_set.append([\n",
    "          instance.drop(target_attr).to_numpy(),\n",
    "          one_hot_encode(encoding, instance[target_attr])\n",
    "          ])\n",
    "\n",
    "    # Faz o parse do dataset de teste para o formato de entrada da rede\n",
    "    if n_outputs == 1:\n",
    "      for idx, instance in testing_df.iterrows(): \n",
    "        testing_set.append([\n",
    "          instance.drop(target_attr).to_numpy(),\n",
    "          [instance[target_attr]]\n",
    "          ])\n",
    "    else:\n",
    "      for idx, instance in testing_df.iterrows(): \n",
    "        testing_set.append([\n",
    "          instance.drop(target_attr).to_numpy(),\n",
    "          one_hot_encode(encoding, instance[target_attr])\n",
    "          ])\n",
    "\n",
    "    net.train(training_set, n_epochs)\n",
    "\n",
    "    for idx, instance in testing_df.iterrows():\n",
    "      inputs = instance.drop(target_attr).to_numpy()\n",
    "      predicted_class = net.classify(inputs, n_outputs, encoding)\n",
    "      actual_class = testing_df.at[idx, target_attr]\n",
    "      if actual_class == predicted_class:\n",
    "        correct_predictions += 1\n",
    "      else:\n",
    "        incorrect_predictions += 1\n",
    "\n",
    "    cost = net.cost(testing_set)\n",
    "    accuracy = round(correct_predictions / testing_df.shape[0], 2)\n",
    "    print('Done! ({0}/{1})'.format(correct_predictions, testing_df.shape[0]))\n",
    "    return accuracy, cost"
   ]
  },
  {
   "source": [
    "## Dataset 1 - house-votes-84.tsv"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('datasets/house-votes-84.tsv', sep='\\t')\n",
    "\n",
    "# Normaliza o dataset\n",
    "df = (raw_df - raw_df.min()) / (raw_df.max() - raw_df.min())\n",
    "\n",
    "training_sample = df.sample(5)\n",
    "testing_sample = df.sample(100)\n",
    "\n",
    "accuracy, cost = test_neural_net(training_sample, testing_sample, attr_list, target_attr, alpha=0.05, lamb=0.5, intermediate_net=[12], n_epochs=5000)\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "print('Cost: {}'.format(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "- Testing fold 0...\n",
      "Done! (19/21)\n",
      "- Testing fold 1...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-763126c534dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mtesting_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'- Testing fold {0}...'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_neural_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintermediate_net\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0mfolds_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f5b5e81fd425>\u001b[0m in \u001b[0;36mtest_neural_net\u001b[0;34m(training_df, testing_df, attr_list, target_attr, alpha, lamb, intermediate_net, n_outputs, encoding, n_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m           ])\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtesting_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/bp_neural_net/neural_net.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, n_epochs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_deltas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_regularize_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/bp_neural_net/neural_net.py\u001b[0m in \u001b[0;36m_update_gradients\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mneuron\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m           \u001b[0morigin_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m           \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0morigin_activations\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36minsert\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(arr, obj, values, axis)\u001b[0m\n\u001b[1;32m   4569\u001b[0m             \u001b[0;31m# very different from a[:,[0],:] = ...! This changes values so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4570\u001b[0m             \u001b[0;31m# it works likes the second case. (here a[:,0:1,:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4571\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4572\u001b[0m         \u001b[0mnumnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4573\u001b[0m         \u001b[0mnewshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnumnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmoveaxis\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mmoveaxis\u001b[0;34m(a, source, destination)\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0mtranspose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1428\u001b[0;31m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_axis_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'source'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1429\u001b[0m     \u001b[0mdestination\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_axis_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'destination'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mnormalize_axis_tuple\u001b[0;34m(axis, ndim, argname, allow_duplicate)\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;31m# Going via an iterator directly is slower than via list comprehension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raw_df = pd.read_csv('datasets/house-votes-84.tsv', sep='\\t')\n",
    "\n",
    "# Normaliza o dataset\n",
    "df = (raw_df - raw_df.min()) / (raw_df.max() - raw_df.min())\n",
    "\n",
    "# Reduz o dataset pra fins de teste\n",
    "df = df.sample(100)\n",
    "\n",
    "target_attr = \"target\"\n",
    "n_folds = 5\n",
    "\n",
    "# Gera conjuntos estratificados para a validação cruzada\n",
    "folds = k_folds(df, target_attr, n_folds)\n",
    "attr_list = read_attr_list(\"datasets/house-votes-attr.csv\", \";\")\n",
    "folds_accuracy = []\n",
    "\n",
    "# Teste da rede neural para cada um dos folds\n",
    "for i in range(len(folds)):\n",
    "  training_df = pd.concat(remove_index(folds, i))\n",
    "  testing_df = folds[i]\n",
    "  print('- Testing fold {0}...'.format(i))\n",
    "  accuracy, cost = test_neural_net(training_df, testing_df, attr_list, target_attr, alpha=0.05, lamb=0.5, intermediate_net=[12], n_epochs=5000)\n",
    "  folds_accuracy.append([accuracy])\n",
    "\n",
    "# Agregação dos Resultados\n",
    "stats = pd.DataFrame(folds_accuracy, columns=[\"Accuracy\"])\n",
    "desc = stats.describe()\n",
    "results = pd.concat([stats, desc.loc[['mean', 'std']] ])\n",
    "results"
   ]
  },
  {
   "source": [
    "## Dataset 2 - wine-recognition.tsv"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "- Testing fold 0...\n",
      "Done! (7/10)\n",
      "- Testing fold 1...\n",
      "Done! (9/10)\n",
      "- Testing fold 2...\n",
      "Done! (9/10)\n",
      "- Testing fold 3...\n",
      "Done! (7/10)\n",
      "- Testing fold 4...\n",
      "Done! (9/10)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Accuracy\n",
       "0     0.700000\n",
       "1     0.900000\n",
       "2     0.900000\n",
       "3     0.700000\n",
       "4     0.900000\n",
       "mean  0.820000\n",
       "std   0.109545"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.700000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.700000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.900000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.820000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.109545</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "raw_df = pd.read_csv('datasets/wine-recognition.tsv', sep='\\t')\n",
    "\n",
    "target_attr = \"target\"\n",
    "n_folds = 5\n",
    "\n",
    "# Normaliza o dataset\n",
    "df = (raw_df - raw_df.min()) / (raw_df.max() - raw_df.min())\n",
    "df[target_attr] = raw_df[target_attr]\n",
    "\n",
    "# Reduz o dataset pra fins de teste\n",
    "df = df.sample(50)\n",
    "\n",
    "# Gera conjuntos estratificados para a validação cruzada\n",
    "folds = k_folds(df, target_attr, n_folds)\n",
    "attr_list = read_attr_list(\"datasets/wine-attr.csv\", \";\")\n",
    "folds_accuracy = []\n",
    "\n",
    "# Teste da rede neural para cada um dos folds\n",
    "for i in range(len(folds)):\n",
    "  training_df = pd.concat(remove_index(folds, i))\n",
    "  testing_df = folds[i]\n",
    "  print('- Testing fold {0}...'.format(i))\n",
    "  accuracy, cost = test_neural_net(training_df, testing_df, attr_list, target_attr, n_outputs=3, encoding=[1, 2, 3], alpha=0.005, lamb=0.25, intermediate_net=[5], n_epochs=1000)\n",
    "  folds_accuracy.append([accuracy])\n",
    "\n",
    "# Agregação dos Resultados\n",
    "stats = pd.DataFrame(folds_accuracy, columns=[\"Accuracy\"])\n",
    "desc = stats.describe()\n",
    "results = pd.concat([stats, desc.loc[['mean', 'std']] ])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}